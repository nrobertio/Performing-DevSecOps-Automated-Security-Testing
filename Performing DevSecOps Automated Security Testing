Performing DevSecOps Automated Security Testing. I'm a former software developer and now a lead penetration tester and DevSecOps consultant, working for multiple companies around the globe. Are you ready to start performing DevSecOps security testing? Could your pipeline use some more security tests? Would you like to see demos of security testing tools in action? Well, then this course is for you. You'll learn all about performing automated security testing. Some of the major topics that we will cover include how to perform automated security tests, how to integrate numerous security testing tools into your existing build pipelines, how to detect secrets within your code base, existing ones, as well as new ones. By the end of this course, you will know all about performing security testing, and you will be ready to implement and integrate security tests into your build pipeline. Before beginning the course, you should be a little bit familiar with software development lifecycles. Knowledge of Docker is a pre, but overall discourses for anyone interested in automated security testing. I hope you'll join me on this journey to learn about performing automated security testing with the Performing DevSecOps Automated Security Testing course here, at Pluralsight.

Initializing the Setup for Automated Security Testing
Introduction to Performing DevSecOps Automated Security Testing
Hi, and a very warm welcome to this course, Performing DevSecOps Automated Security Testing. My name is Peter Mosmans, and 
I am a lead penetration tester and DevSecOps consultant. Let's take the following scenario. We have Maeve, and Maeve works for a large company, 
still Globomantics. She just watched a course, Approaching Automated Security Testing in DevSecOps, and now she's ready to start using automated security tests. 
She wants to know how she can perform those automated security tests and what kind of tools she can use. She asked for the help of a consultant, Jennifer, 
and Jennifer is going to help her answer those questions. She's going to demonstrate a variety of different tools and demonstrate how those tools can be 
used within existing continuous integration/continuous delivery or deployment pipelines. And that is exactly what this course is all about, Performing DevSecOps
Automated Security Testing. Let's take a look at how the course is structured. In this module, we'll be spending a lot of time initializing everything we need
for performing automated security testing. As the course contains lots of demos, it's important to get the practical fundamentals or our lab properly set up. 
Then, we're going to take a look at automating code security testing. We're going to introduce several tools that you can use in your build pipeline, 
how to perform automated code security testing. Then, we'll be taking a closer look at automating third party library security scanning, 
followed by automating container security testing. The last module in the course will revolve around automating infrastructure and dynamic application security testing. 
This course demonstrates different types of tools. We're going to show lots of tools. We're going to have lots of demos. Hopefully, after watching this course,
you'll feel more comfortable using those tools yourself. Per tool, we're going to take a look at three different characteristics. 
And by looking at those three different characteristics, we're trying to see where this tool makes sense for you to apply in your current work situation. 
The first thing we're going to look at for each tool is the advantage. Is the tool genuinely useful? Will it give you more information, 
or will it ultimately lead to a better secure system? The second characteristic we're going to look at is compatibility.
Is it easy to use in your setup? Does it fit with the language and framework you're using? And the third characteristic
we're going to look at is the trialability. In other words, is it easy to try out? Do you need to invest in new hardware or new techniques 
or new knowledge in order to try out the tool? And hopefully, by presenting these three characteristics per tool, this will make it easier
for you to pick from this course the right tools that you can apply at your current setup? What's in it for Maeve watching this course? Well,
after watching this course, she'll know more about the different tools that you can use. She'll know more about the various types of
tools that there are, and she'll know how she can start using those tools in the continuous integration/continuous deployment or continuous delivery pipeline.
The course is meant for DevOps engineers. It's also meant for security professionals, as well as developers, product owners, scrum masters, 
so actually anyone interested in automated security testing. But wait, if the course is meant for such a large audience, what kind of technical level
should you have in order to be able to follow along with the demos? Well, the demos gradually build up. We start very simple, setting up the lap,
making it a bit more complex, and on and on it goes. In other words, I advise you to look at all the modules in order so that the demos makes sense. 
That way, probably everybody will be able to follow along. The course you're currently watching is part of the DevSecOps path. 
The first course in this series is Approaching Automated Security Testing, the second course Performing DevSecOps Security Testing,
and the third course in the path is Integrating DevSecOps Security Testing where you learn more about integrating the tooling
into techniques like AWS or Azure. The course you're watching right now is Performing DevSecOps Security Testing. 
In this module, initializing the setup for automated security testing, we're going to talk about automating application and in particular code security testing.
Then, we're going to spend some time describing the demo lab, and we take our time setting up the demo lab so that you can follow along and that you can see how easy it is to implement some of the tools we're going to be showing you followed by demos exactly how you do that. We're going to be setting up the demo lab, and we're going to spend some time setting up a built pipeline. We'll be finishing this module with a summary. In the next clip, we'll be taking a look at automating code security testing.

Automating Code Security Testing
Before we start testing code, I'd briefly like to go over and reuse a slide from the course Approaching Automated Security Testing in DevSecOps and look at exactly what you can test. Obviously, you can test code, and testing code is what we will do in this module. You can also test containers, which we shall do in another module, and you can test infrastructure, the last module of this course. Right now, we're focusing on testing code. And when testing code, what exactly can you test or what should you test? There's readability. Is it easy for other developers to read your code? And this is very important because code is written once, but read many times. And the more readable code is, the easier it is to spot potential security vulnerabilities. You're also looking for maintainability and clarity. You do not want spaghetti code with code that is easy to reuse and again to read for other people, which decreases the number of potential security vulnerabilities. You're looking at insecure patterns, for instance incorrect boolean statements when logging on. You're looking for hard‑coded secrets. You don't want secrets to be buried in your code, especially not unencrypted. And you're looking for insecure third party libraries. The first three items on the list, readability, maintainability and clarity, and insecure patterns is what linters can do, which will be demonstrated in various demos. Then we'll be taking a look at how we can detect hard‑coded secrets. This is also something that we shall demo in this module. Code quality systems are especially excellent to detect for readability, maintainability and clarity, and trying to detect insecure patterns. And yes, this also will be demoed in this module. Insecure third party libraries will be demonstrated in the next module. In this module, we'll be focusing on using linters, detecting secrets, and using code quality systems. Another brief recap on how to approach automated security testing and why a recap? Because this is really, really important, especially when you start to applying performing those automated security tests. When possible, let the team itself pick its own tools. Don't try to impose specific tooting to use, but make sure that everybody feels comfortable with choosing their particular tooling. Start with quick wins first. This is something that hopefully you'll take away after viewing this course, that you see how easy it is or how difficult it is to try certain tools. The triability factor, as mentioned in the previous clip. Invest time in setting a baseline. Again, as you shall see with the demos, some tools need lots of tweaking and tuning, and some tools are almost ready to go. Don't use hard quality gating out of the bed. Make sure that the tools run for a while before you actually start using them to mark builds as failed when it fails one or more off the several security tests. And plan time to configure and weed out false positives. Make sure that all information that is presented to the team is actually useful actionable information. These three items are really important, not using hard quality gating up front, planning the time to configure the tools, and investing time into setting baselines. In the next clip, we're going to describe how we set up the demo lab.

Describing the Demo Lab
As we shall spent a lot of time in the demo lab during the demos, it's important to see how the demo lab is set up. It consists of a Git server. And for the Git server, we're using the open source solution GitLab. Then for our continuous integration or build server, we're using Jenkins, another open source solution. And for our Docker registry, we'll be using the Docker registry image. Everything will be running on a server. And on that server, we have Docker, as well as Docker Compose installed. If you're not familiar with Docker, I wholeheartedly recommend watching one of the numerous Docker courses on Pluralsight. Docker Compose is a tool which will help us set up multiple containers at the same time. So all these three machines, GitLab, Jenkins, and Registry all will run as Docker Container on our server. And on our server, we'll map port 80 for HTTP traffic for our GitLab, port 7722 SSH traffic to GitLab in order to commit code to our test repository for the Jenkins for test repository, and we will map this as gitlab.demo.local. For the Jenkins Server, we will map port 8080 in order to see the web interface, and we'll use the host name jenkins.demo.local. And for the registry server, we will map the default registry port 5000, and we'll use the host name registry.demo.local. And, of course, we will have a client, which will push code to GitLab. So what will be the workflow for all of our demos? Well, we'll push code to the GitLab Server. Then GitLab will trigger a webhook on Jenkins, which will subsequently execute a build or a job. And that job will pull code from GitLab, which we just pushed to GitLab and execute our build scripts or our pipeline. Now in order to make everything work in our lab, we need to set some access permissions. What kind of access permissions do we need to set in order to make this workflow as smooth as possible for our demos? Well obviously, our client needs to be able to commit code to the Git Server. Therefore, our client needs to have permissions to push code. We would like Jenkins, our build service, to be able to set webhooks on GitLab. This makes it easier when creating jobs on Jenkins that that job actually sets a webhook that will be fired by GitLab. This will probably become a bit more clear during the first demos. And obviously, Jenkins needs to be able to clone the repository. It needs to be able to pull the code from GitLab in order to perform a build. So those are the access permissions we will be setting in the first demo. All of the demos in this course revolve around two projects. The first one is a Docker base image project, and it basically contains a Docker file meant to create a Docker image, and the Docker image itself contains security testing tools, tools that we will be using throughout the demo. The source code for this project can be downloaded from GitHub. The second project we'll be using as a demo is a Node.js web shop project, and this is a deliberately insecure web shop. And the reason we will be using it is because it's a basic and standard Node.js application. The source for this project also can be downloaded from GitHub from the location shown at the slide. If you want more information about the tools we will be using in this module, the first one is from GitLab, gitlab.com. The second one is from Jenkins, jenkins.io. Then for Docker registry on docker.com and then the location for lab files, which you might find useful as we'll be using Docker Compose can be downloaded from the DevSecOps repository, and the first demo project can be found under the Tools image repository. Let's hop over to the next clip where we'll be having our first demo, setting up the demo lab. See you there.

Demo: Setting-up the Demo Lab
In the first demo of this course, we will be setting up the demo lab. And in order to do that, we're going to run and configure GitLab, our Git server, we're going to run and configure Jenkins, our build and continuous integration server, and we're going to run the Docker registry, a location where we can push our Docker images to. The prerequisites are Docker, which is installed, as well as Docker Compose, which is usually installed by default when installing Docker. The demo lab contains of a standard Debian Linux installation. First, let's see whether Docker is properly installed. And in order to check that, I'm going to do a which docker. And yes, we can see it can find the binary Docker, and we are going to run the hello world image, which is being performed by docker run hello‑world. And if this property succeeds, then I know that Docker is properly set up. Perfect, this works. Prerequisite number 1. For the second one, which was docker‑compose, which docker‑compose to see where we can find an executed binary. Yes, that is also working. By the way I'm using the Z shell, zsh, with several plug‑ins, and I frequently clear the screen during the demos using the shortcut Ctrl+L. Let's clone the repository that we need for this lab, which can be found on github.com/PeterMosmans/devsecops‑lab, then cd to the directory, cd devsecops/lab. And then I'm going to check out the tag demo_1 because that's the version we will be looking at right now. And just for cosmetic purposes, I'm going to create a new branch named demo_1, which will be shown in the zsh prompt. In order to view the docker‑compose file, I'm going to use the utility bat, which is the same as cat, but then with syntax highlighting. And here you can see the docker‑compose file that we will be using for our first demo. If you're not familiar with the Docker Compose syntax, I recommend to watch a course on this on Pluralsight. But basically, it defines multiple Docker services that will be run. In the top, we define the network that we're going to use, and we named the network lab Then you'll see the definition of the first service, which is GitLab, and GitLab uses the image gitlab/gitlab‑ce. Then we set the hostname that we're going to use and we expose the ports 80 and 7722, 80 for the web interface and 7722 for SSH access or Git access. The next service that we're going to define is Jenkins. And for Jenkins, we're going to pull the image jenkinsci/blueocean, and we're going to map port 8080. So on this port, the web interface will be accessible on the name jenkins.demo.local. And the third service that is defined in the docker‑compose file is the registry service, and the registry service uses the image registry with the tag 2, and we expose port 5000. So on the defined lab network, the registry service will be available on the registry.demo.local port 5000. At the end of the docker‑compose file, you see several volumes that the services use. So this is the docker‑compose file that will fire up three services, GitLab, Jenkins, and the registry service. Let's clear the screen. And with the command docker/compose up ‑d for detach, we start the three services that are defined in the docker‑compose.yaml file. It's now pulling in the three images that we defined, and the services will then start running in the background. There are still three other things we need to do. As shown in the previous clip, we need to set some permissions. I already generated a Jenkins user account and generated an SSH key pair for it. Jenkins needs to be able to pull source code from the GitLab server, and I'm going to copy the private key file, as well as the public key file. And the private key file, we'll be using in the Jenkin server, and the public key file will be used for the GitLab server. First, let's get the private key file, which is located under jenkins/.ssh/id_rsa. This is the default location for SSH keys on the system. And I'm also going to copy the public part, which is the same name, but then with the extension .pub. So I copied both files. And the final part we need is the initial admin password of our Jenkins Docker instance. And you can find that by catting a file, which Jenkins automatically sets up when being run for the first time. And I'm going to cat that using sudo cat and then the location of the Jenkins volume /var/jenkins_home/secrets/initialAdminPassword. This is something you need when running Jenkins for the first time as a Docker container. That's the only reason we need to write that. I'm going to do a docker ps, which shows the running Docker containers. And as you can see, all three services have been started successfully. So let's now go to the web interface to configure Jenkins and GitLab. Fire up a browser and go to the address gitlab.demo.local because that's what our GitLab instance is listening to. Now we need to set an initial password for the root account, and, of course, I'm going to choose a strong password here, even though it's just a demo instance. We apply it. No, I don't want to save the password. And now we can log on using the username root and the password we just chose. Click on the Sign in link, and if everything goes well, you log in to the fresh GitLab instance. Now let's go to the upper‑right corner to Settings. First, go to SSH Keys at the left‑side menu. Then, we paste in our personal public SSH key so that we can push code ourselves to the Git server. And clicking on Add key, and our SSH key is added. Next, go to the menu item Access Tokens, and we will generate a new API key. The API key will allow Jenkins to set webhooks on GitLab. So this makes our workflow a little bit easier. Add a name for the access token and click on API for Scopes, then Create personal access token, and we're going to copy the new personal access token value because we need to add it to Jenkins later on. Select the wrench in the top‑left corner to go to the administrative menu, then go to Settings, then go to Network, expand the outbound requests, and then make sure that the checkbox Allow requests to the local network from web hooks and services is checked and click on Save changes. This allows Jenkins and GitLab to communicate with webhooks. Almost done with GitLab. Now go to the menu item Deploy Keys. The final step in configuring GitLab is adding a deploy key. This allows Jenkins to pull code from GitLab. Then, click on New deploy key. And here we paste the public key from Jenkins that we just copied in the console. This will allow Jenkins, the build server, to pull code to clone code from GitLab. Let's name it that Jenkins read only deployment key, click on Create, and there we go. Now we're all set with regards to GitLab. Let's go to Jenkins. We map the web interface of Jenkins on jenkins.demo.local port 8080. And if we open that address, we will be greeted by the first screen of Jenkins. And this is where we paste in the initial admin password that we grabbed earlier on during the console session. No, we don't want to save this password, and we select Install suggested plugins. To be honest, we definitely don't need all those plugins, but this is easier to set up. And we're looking at the demo lab, nothing more, nothing less. After this process is ready, we can generate a Jenkins user. And we fill in our username, a strong password, and what you also need to fill in here is an email address. Then click on Save and Continue. It pre‑populated the Jenkins URL. Click on Save and Finish, and Jenkins is ready. Click on Start using Jenkins, and you will be greeted by the Jenkins web interface. Click on the left side on Manage Jenkins. Then dismiss the notification that you'll see. This is not important for the demo. And then click on Manage Plugins. There might be some updates ready to be installed. Click on Select All and then on Download and install after restart, and it will update the plugins. To be honest, the plugin system of Jenkins is sometimes a bit finicky, and just by reloading the page, it will restart Jenkins, and you can continue using it. We have to install two additional plugins and then, trust me, then we're ready. After Jenkins is restarted, use the username and password that you used to create the account in the first place, sign in, and then click on Manage Plugins at the left‑hand side. Then select the tab Available, then go to the filter field or search field, and enter gitlab branch because we're looking for the GitLab branch source plugin. There are multiple GitLab plugins, but the one we're looking for is the GitLab branch source plugin. Click on Install and then Download now and install after restart. Again, this might be a little bit finicky, so you might have to reload after this. Go to Manage Jenkins, select Manage Plugins, select the Available tab, and search for docker. Similar as with GitLab, there are multiple Docker plugins, and we're looking for the one aptly called Docker. Select the plugin, Download now and install after restart. And after Jenkins restarts, again, go to the menu item Manage Jenkins on the left, then click on Configure System and scroll down. Personally, I unselected usage statistics for security purposes, and then we're going to add the GitLab server. Click on Add GitLab Server, add a display name. And for server URL, we'll be using the URL we configured previously, so that will be gitlab.demo.local. And then we're going to add credentials. The Jenkins server needs to be able to set those webhooks we talked about earlier on. Go to Add credentials, select GitLab Personal Access Token. Paste in the token we generated earlier in the GitLaB web interface. You can add a nice description. You don't need to fill in the ID field. This will be randomly generated by Jenkins. Click on Add. Click on Manage Web Hooks and Manage System Hooks. Select the credentials that we just added, and now click on Test connection. If everything went well, then you see credentials verified for user root. Click on Save and congratulations. We've now configured Jenkins, as well as GitLab. In the next demo, we'll set up our demo projects.

Demo: Setting-up a Build Pipeline For Automated Security Testing
In the previous demo, we took quite a long time sending up the demo lab, and hopefully now you'll see why. In this demo, we'll be setting up the tools‑image project. This is a basic project which will build a Docker image containing automated security testing tools. In the demo, we'll first be creating a GitLab project to host our source code. Then, we will create a Jenkins project, and that Jenkins project will automatically build the tools‑image for us as soon as we push code to our GitLab repository. Let's briefly go over the workflow. On the left‑hand side is the client. We will push code to the GitLab server. Then, GitLab will trigger a Jenkins job. Jenkins will execute a build, and we'll pull code from the GitLab repository. In our job, we will create a Docker image, and the Doctor image will be pushed to our registry server. Let's start with going to GitLab web interface. I'm still logged on, and I click on Create a project. Here I'm going to enter the project name, tools‑image, and click on Create project. Now we only need to enable the Jenkins key so that Jenkins is able to clone this repository. In the menu on the left side, go to Settings, Repository, and then expand the deploy keys. If you scroll down here, you click on Publicly accessible deploy keys, which we already add it, and enable the key. Keys are not enabled by the fault. Hence, you need to enable a deployment key per new repository. We're all done for GitLab. Now switch over to Jenkins. Log on using our username and password, and we're now going to create a new Jenkins project or a Jenkins job. Go to the left‑side menu and click on New Item. I'm going to use the tools‑image item name. And if you scroll down, I'm going to select a multi‑branch pipeline. Click on OK. The multi‑branch pipeline can build from multiple branches. For display name, I'm going to use the same name, tools‑image, and now we need to add a source where Jenkins can pull the source from. If you click on Add source and then select GitLab Project, then you'll see the fields are already pre‑populated for the GitLab server. However, we haven't added the actual private key of the Jenkins account. Click on Checkout Credentials, add Jenkins, and then select SSH Username with private key. And here we're going to paste in the private key from the server. I'm going to add a simple description. You can leave the ID field empty. For username, we're going to select root because that's the user account we used on the GitLab server, and here we add the private key from the server. And I'll click on Add. Jenkins now will be able to clone the source code from GitLab. Select the credentials we just added. For owner, fill in root. And now, if you click on the drop‑down item, you'll see that Jenkins is able to connect to GitLab. It can read the repositories that are accessible for Jenkins. Let's select the tools‑image repository. And if you scroll down, you'll see that Jenkins, by default, uses a Jenkins file. This Jenkin file is a Groovy script and defines the pipeline. Don't worry too much about the syntax right now. Click on Save. It's immediately starting to scan the repository, so the connection seemed to work. But as the repository is empty, nothing else happens. Let's now go back to the console. Let's clone our demo repository from GitHub. I'm going to the tools‑image directory, and I'm going to check out demo_2. I'm going to create a branch for it, clear the screen, and this repository contains a number of files. What we're interested in is the Dockerfile, so let's take a look at that. This is a standard Dockerfile, which will create a tools‑image, an image containing several security testing tools. Don't worry too much about the syntax, but we're going to use this image because it's going to contain security testing tools. Another file we're interested in is the Jenkinsfile. This Jenkinsfile contains the pipeline configuration, and this pipeline will consist of two different stages. One builds and tests the image, and the other stage will push the image to the registry. Again, don't be too concerned about the pipeline syntax. What's important to know is that this script will define a job on Jenkins to say build an image, test the image, and push the image to the registry. We're now going to switch the origin over because we want to push this code, which we just downloaded from GitHub to our own GitLab server. So by performing this command, the git remote set‑url command, we're changing the URL of the source, and now we will be able to push our clone to our GitLab server. We're using the command. git push origin master. This is the first time we connect to the GitLab server. I'm going to accept the host key. and it now pushed our code to the master branch. To summarize, we cloned the repository from GitHub, we changed the origin, and we pushed the code to our own GitLab server. Let's switch back over to the Jenkins web interface. And lo and behold, Jenkins detected changes in the GitLab repository and immediately started building the project. I'm going to click on tools‑image, and here you can see it's currently building our branch. I'm going to click on master to get a more detailed pipeline view. And here you can see, it checked out the code from GitLab, it's now building our Docker image, testing it. And if everything went successful, it will automatically push it to our registry server. Even though it might look like a lot of work we did in the previous demo, now you see where it pays off. It's now very easy to test changes, changes in the code, as well as changes in our pipeline. And our job finished successfully. The image has been pushed to our Docker registry service. This course is about performing automated security testing, and now we're able to do that. Let's get back to some theory. In the next clip, we'll be taking a look at linters and what they can do for us.

Module Summary
The module summary and a short one because we haven't actually performed any automated code security testing yet. We have been looking at setting up a simple automated pipeline. Things that are important to keep in mind. are don't use hard quality gates up front, plan time to implement tools, and invest plenty of time in setting up a baseline. Now we've covered the basics or setting up our lab. Let's go back to Maeve and Jennifer. Maeve says, now I know how to set up a testing pipeline. Jennifer is happy. She can't way to actually perform automated code security testing. So she says, let's start with security testing code. That is what the next module will be all about. See you there.

Automating Code Security Testing
Introduction
Automating Code Security Testing. In our scenario, we have Maeve, eager to start with automated security testing, and we have consultant Jennifer. Maeve asks, where should we start when performing tests? And Jennifer says, well, let's start with testing our own code. Using the right tools at the right place can make a lot of difference. In the grander scheme of things with regards to automated security testing, we're now looking at performing automated security testing, and the performing section contains of testing code, third party libraries, containers, and infrastructure. Right now, we're going to look at testing code, and this code module is divided up into three different segments. We are going to take a look at linters, at detecting secrets, and we're going to take a look at testing for code quality. And we will start this module with looking at linters. So you already know the first item on our module overview, and that is using linters, of course, followed by a demo on how to use a linter. Then we'll be taking a look at detecting secrets with several demos. We shall start with searching in a repository for existing secrets. We're going to use pre‑commit hooks in order to prevent secrets from entering our code base, and we're going to detect secrets in a build pipeline. Then we're going to take a look at using code quality metric systems, of course with several demos. We shall install and use a code quality metric system and look at the results. And we will end this module as usual with a summary, but that didn't even fit on this slide. Let's dive right in with using linters..

Linting Code
Linting Code. Using linters, what exactly can linting do for you, and especially with regards to security? Well, linting can detect errors in your code, and errors can lead to security vulnerabilities. Linters can also detect formatting or styling issues. And I cannot stress this enough, readable code breeds more secure code. Linters can suggests best practices, thereby it increases the overall quality of the code. And because the code is more opinionated because everybody follows the same linting rules, it makes maintenance of code easier. If you see all those advantages, why shouldn't you use a linter or what are known issues with linters? Well, not every language has quality standard linter tools available. What we mean by that that each language, each framework usually has one or several linters. However, not all linters are the same. And some languages exist or some frameworks where there aren't any linters available. Different versions or configurations of linters can lead to different results. Deciding upon which linter to use, which version, and which configuration file is very important because if people are using different versions or, worse, different linters, then it all negates the advantages you might have from linters. Some linters are very verbose, and information overload can lead to focusing on less important or unimportant issues. You will never have the perfect issue‑free code, so make sure you're focusing on the right thing and not get distracted by the information overload that a linter might generate. I'd like to show another slide from the course Approaching Automated Security in DevSecOps, the shift left paradigm. The standard software development life cycle looks like defining, designing, developing, deploying, and maintaining. What shift left or security shift left or shift left security means is instead of focusing on the last part when code is already in production and performing only penetration tests, you try to shift security more to the left, for instance during a deployment phase or just before a deployment phase by performing dynamic application security testing. Another phase where you can do that is during the develop phase by using static application security testing tools. The sooner you find bugs in your code, the cheaper and easier to use to fix the bugs. Let's take a workflow from developer Dave. Before Dave commits code he's writing, he triggers a pre‑commit hook locally. Then code is committed to his local repository. It's pushed or committed to the main repository. From the Git repository, it will flow to the build server, in our instance this is the Jenkins server. And when artifacts are created, it will push it to a registry server and from the registry server usually it will be deployed. This is a generic workflow. Where exactly can you perform linting? You can perform it in the pre‑commit phase, so locally before actually committing code to your local repository to your local clone. Another phase where you often see linting is during the build phase. So here the build server pulls the code from the Git repository and performs linting on it and reports back that results from linting phase. The tool that we will be demoing in the next demo will be a linter for a Docker file called hadolint, and it's going to lint our Dockerfile and report back to us in the next clip.

Demo: Linting a Dockerfile
In this demo, we will be linting a Dockerfile and, specifically, our Dockerfile from the tools‑image project. We will first use the linter on the command line. Then, we'll be using the linter in a build pipeline, and we're going to modify the Jenkins jobs so that it conditionally pushes the Docker image. When the linter will present us with warnings or errors, the job will fail. Let's go to our console. We are in our tools‑image repository, and this repository contains, as we saw in the previous demo, a Dockerfile. In order to lint the Dockerfile, we're going to use a Docker container, which contains the linter. So I'm going to use the docker command. I'm going to run an image. I'm specifying the rm flags. I want it to remove the container after use. I'm going to map the current folder to the /work folder, and I'm going to pass it read‑only flags. Then I'm going to pass in which directory the Docker container should start /work, then the name of the container, hadolint/hadolint, and then, and that's very important, the version number, the version tag, because, as we saw in the previous clip, vendors sometimes change their configuration between version numbers. So you should always use version numbers when using linters Then the name of the linting command, hadolint, and then the name of the file we're trying to lint, Dockerfile. It's pulling in the image, and it's running hadolint. And as you can see, it came back with three warnings. So apparently we can improve our Dockerfile. I'm going to clear the screen. And now, because we have pipeline as code, we have the Jenkins file, we're going to edit the Jenkins file, and we're going to add the hadolint linter into the build process. I'm using Emacs, an editor. You can use any editor, of course. And within the script, I'm going to define a new stage. Let's call the new stage lint, and within this stage, we want to use a Docker container, the same container we just used on the command line, that hadolint tool. And once again, don't forget to specify the specific version you're linting with or against. The reuseNode true is a flag for Jenkins, which make sure that the workspace is preserved between stages. And what do we want to do in this specific stage? Well, we want to do exactly the same as within the command line. We want to run the hadolint executable on the Dockerfile. So I'm going to define a shell script with the label Lint Dockerfile. And the command that should be executing is hadolint Dockerfile. And it should concatenate the output to a text file, which we can show as a build artifact. We're going to name it hadolint‑results.txt. So this will perform the exact same command as with on the command line and redirect the output to the file hadolint‑results. I'm going to save the file, and now let's go to the bottom of the file because we now also want to save that artifact that file hadolint‑results. Here we're defining a pipeline stage, which you should always run regardless whether the build failed or not. And what it should do is save the artifacts that we created earlier on, hadolint. And for this, you can use the Groovy syntax archive artifacts and specify a regular expression. I want to save all files ending with results.txt. Save this file and exit Emacs. Let's now create new branch for our code, git checkout with the ‑b flag in order to specify a new branch, and we're going to call demo_3. We're going to add the Jenkins file that we just modified, git add Jenkinsfile, and we're going to commit the file and specify a commit message. Now we're going to push the code to our GitLab server, and we're going to push it to the new branch demo_3. We push the code, and now if everything went correctly and if you go back to our web interface, I click on tools‑image in the left‑upper corner to see the latest results. And, yes, it detected a new branch, demo_3, and the build failed. Is that bad? Actually, no, this means it worked. We added the linter hadolint, it detected some warnings or errors in our Dockerfile, and thereby the build was failed. No Docker image was pushed. I click on demo_3 for more information. We can download the build artifacts, hadolint, and we can see exactly what went wrong. And these are the same results we saw with running at the command line. This was an example of adding a linter as an automated security testing tool. Let's look at the workflow and summary of using linters in the next clip.

Workflow and Conclusion of Using Linters
The workflow for using linters. First, one needs to agree upon the tooling, exactly what kind of linter you'll use and the version that you'll be using. Then create a list of current linter issues, just as we've done with hadolint. Audit the list of issues and check whether it's actually a false positive or a real issue. We haven't done that so far with our demo, but we will do that later on. Add the configuration file to the repository to make sure that the linter only complains about valid issues. And use that configuration with every scan. Warn or fail a build for new issues that arise and update the configuration when necessary. Then let's look at the advantages of using linters. Linters can detect errors, formatting and styling issues, and suggest best practices. The advantage of using linters is that it improves readability, and it improves consistency throughout code. Compatibility‑wise, that depends on the linter. Not all linters might be compatible with the tools or with the frameworks that you've currently be using. And how easy is it to try out a linter in your current pipeline. On average, linters are very easy to employ in continuous integration pipelines as you hopefully saw in the previous demo. All tools that you might want to implement as security testing tools should follow the same iterative process. First, select the tool, then implement it, analyze the results, and then improve, and improve also means tweaking or tuning the tool. As we saw in the demo, our build failed. But should our build be failing or should we configure the baseline, the configuration of hadolint? For more information, check out GitHub for hadolint and the demo project that we started using, tools‑image. In the next clip, we're going to talk about finding secrets in code.

Detecting Secrets in Code
Detecting Secrets. Why would you want to detect secrets? Secrets should never be hardcoded into your source code. And if secrets aren't hardcoded in your source code, ensure that secrets are encrypted. They should never be unencrypted. Regardless, it's a bad idea to put stuff like variables and especially passwords and secrets in source code. Especially now, more and more companies are open sourcing their products and pushing source to platforms like GitHub, you want to make sure that your history does not contain inadvertent secrets. And if your source code contains secrets, at least validate them so that you know where the secrets in the source code are. Where and when can you detect secrets in your workflow? Well, the best location is the pre‑commit location. This ensures that before a secret actually enters your code base, it is intercepted, and the developer or to commiter gets a message from hey, this is a secret which probably should not end up in your source code. Are you sure you want this to add? Another location is the build server or the build process. The build server retrieves source code, which is already committed, and then it can analyze the source code where it contains new secrets or when it contains known secrets that the secrets are actually validated or audited. These are the two best locations to find secrets in your source code. We will be demoing the following tools. TruffleHog, and truffleHog is a tool that searches through Git repositories in order to detect secrets. And not only secrets in current code, but also during and through between commits of that repository. Then we're going to demonstrate pre‑commit, and pre‑commit is a framework which makes it really easy to manage pre‑commit Git hooks, exactly the best location where you can prevent secrets from entering your code base. And we're going to demonstrate detect‑secrets. Most tools deal with new secrets. Detect‑secrets is a perfect tool to also deal with existing secrets in your code base. In my personal opinion, it's currently one of the best tools to detect secrets in the continuous integration pipeline. In the next clip, we will start with demoing truffleHog.

Demo: Detecting Existing Secrets in Code
In this demo, we will be detecting existing secrets in a code base. We're going to locally install the tool truffleHog. Then we're going to run truffleHog on the tools‑image project. We're going to clone the second project, which we'll be using throughout the course, juice‑shop. And we'll be running truffleHog on the juice‑shop repository. Let's go to the console. TruffleHog is available as a Python library, and Python libraries can easily be installed by pip, the Python package manager. I'm first going to check what I have with pip3, the Python package manager for Python 3. And then I'm going to install truffleHog locally by performing a pip3 install and then the name of the library, truffleHog. Okay, it installed it correctly. Let's clear the screen and now run truffleHog on our existing tools‑image project. And we do that by running the command trufflehog, the binary, and then the name of that repository, which, in our instance, is named tools‑image. It didn't find anything. Wait, weren't there any secrets in the Dockerfile? Let's take a closer look at the Dockerfile using the utility we've used before, bat, cat with syntax highlighting. I'm going to scroll to the end of the file. Can you detect the secret here? So this is an example of a secret, which it didn't detect. There is not a tool available which will detect all secrets or at least not without giving loads of false positives, which makes the tools also unusable. Be aware of that. I'm going to exit bat, clear the screen, and now let's start with our second project, the juice‑shop. I'm going to perform the git clone command with the name of the repository, its host on GitHub, bkimminich/juice‑shop. And it's now cloning the whole repository. So let's run the truffleHog command again, but then with this repository, truffleHog juice‑shop. Whoa, and remember, truffleHog doesn't only find secrets in current code base, but it also looks through the Git commit history, all of the previous commits where sometimes people committed a secret and then referred to that commit or actually tried to refer to that commit. Given the distributed nature of Git, it's almost impossible to actually remove something once it's committed and out there. As you can see, it found a lot of secrets. So here, the tool truffleHog found in some commits either added secrets or removed secrets, but something which you should be aware of. And of course, there might be lots of false positives. I'm going to interrupt this process. I'm going to clear the screen. If you run the executable truffleHog and you feed it the ‑‑help command, you'll see the several other parameters that you can use. But wait. What if you don't have Python installed on your computer or you don't want to install Python on your continuous integration server? Well, actually, the previous project, the tools‑image project, creates a Docker image containing lots of tools, and one of those tools is truffleHog. So we can also run truffleHog from a Docker container. And in order to do that, we type in the command docker run with the flag ‑‑rm. And we map the current directory, /juice‑shop, where we just downloaded the juice‑shop project, and we map it to the local folder /src:ro. Then, we specify the location of our registry server, registry.demo.local port 5000 and then the name of our image, /tools‑image. Then we're going to run the binary truffleHog, and we want to run it on the /src directory, which is mapped to our juice‑shop clone. So by executing this command, we effectively do the same as running truffleHog locally. This makes it easy to run those tools in a continuous integration environment from Docker containers. So far, truffleHog looks like a good candidate to run over existing code bases and to see whether it can detect secrets in your code base. In the next demo, we're going to look at some more advanced examples in combination with the pre‑commit framework.

Demo: Preventing Secrets from Being Committed
In this demo, we're not only going to detect existing secrets, but also new secrets. We're going to install and run the tool detect‑secrets, we're going to install the pre‑commit framework, and we're going to configure pre‑commit. Let's hop to the console. Detect‑secrets is also a Python library and can install the same way we did with truffleHog, pip3 install detect‑secrets. By the way, we could also run detect‑secrets from the Docker image, and we will do so in the next demo. I'm going to clear the screen, and let's go to the repository of juice‑shop. Now in this repository, we can run the tool detects‑secrets, similar as to what we did with truffleHog. By supplying the command scan, detect‑secrets will try to detect secrets. However, as you might find out, this will take a long time. Detect‑secrets takes up more computing time than truffleHog. I'm going to interrupt the process. If you look at the juice‑shop repository, it contains a lot of files. One of the folders is the test folder, and we choose not to scan for secrets in the test folder. Another folder is the front end folder, and we don't think it's a good idea to scan that either. So we'll both exclude that from our scan. So we're going to perform a detect‑secrets scan, and then we're going to exclude files so you can use regular expression syntax. So we're going to skip everything in the front‑end directory and everything in the test directory. We're going now to concatenate the output into a file, a .secrets.baseline.json file, and this file will contain the scan results, as well as the settings used for this scan. This is a file that can be added to a repository. Now the scan is pretty efficient or pretty fast, I should say. And let's use bat again to open the JSON file to see exactly what the tool did. So this file contains configuration for the detect‑secrets tool with several parameters that you can use, and it also contains the secrets. But not just the secrets, hashes of the secrets, so the file itself will not contain secrets. That would kind of defeat the purpose. Now the tool generated kind of a baseline. And the next step in the process for using detect‑secrets is auditing the baseline. So we're going to use the command detect‑secrets, audit, and then the name of the baseline file we just made. And now the tool allows us to go one by one through its results and to flag whether it's a false positive or whether it's actually a secret. And this clever system allows us to make a baseline of the current situation. So instead of having to roll over all of our secrets at once, we now have a baseline. We now know where we stand and what we need to do in the future when we want to roll over all of our secrets. And I'm going to quit auditing the secrets. The next step is installing the pre‑commit framework. What can we do with the pre‑commit framework? We're going to automatically add hooks, which will detect whether we want to add secrets to our Git repository. Pre‑commit is also a Python library, and the same as with the previous Pyfrom libraries, we're going to install it using pip3 install pre‑commit. The tool itself is installed. Now we need to install the hooks for our juice‑shop repository, and you can install the hooks automatically by doing pre‑commit install. Now we need to tell pre‑commit which kind of hooks we want to use, and it uses a configuration file, a YAML file. So I'm going to open that file using Emacs. The name of the configuration file is .pre‑commit‑conflict.yaml. The syntax of this configuration file can be found at the site of the pre‑commit framework. I'm using the syntax as recommended on GitHub. It contains the name of the hook, as well as the name of the secrets baseline that we just created. And this specific configuration file can also be found in the DevSecOps lab repository on GitHub. The goal of this course is to show as many tools as possible, not to dive too deep in the specific configuration of any tools. Let's save this file and exit Emacs and clear the screen. Let's now test whether our pre‑commit and detect‑secrets hook is working. Let's create a text file containing a password. Let's use the string secret password, and let's name the file setting.txt. Now let's stage the file by using the git add command, and let's try to commit. The first time the pre‑commit runs, it will install some stuff in the background. And after that's finished, you can see the pre‑commit hook successfully detected that we were trying to commit a secret to the repository. Demonstration succeeded. In the next demo, we will add detect‑secrets to our continuous integration pipeline.

Demo: Detecting New Secrets during Automated Security Testing
In this demo, we will be detecting new secrets during automated security testing. And in order to do that, first we will be setting up a pipeline for the juice‑shop project, and then we will use detect‑secrets in our Jenkins pipeline. First, let's go to the GitLab web interface in order to create our juice‑shop repository. In GitLab, we're going to create a new project, and we're going to name the project juice‑shop. Click on Create project, then go to Settings, Repository, and the same as with the tools‑image project, we need to enable the deploy keys. Click on Expand, scroll down, and we're enabling the Jenkins key. This enables Jenkins to clone this repository. We're done with GitLab. Now let's go to Jenkins. Let's create a new item or job from the menu on the left. We're going to call this job juice‑shop, and we're going to select a multi‑branch pipeline. The display name will be juice‑shop as well, and let's add the GitLab source. Use the same credentials as last time, the Jenkins read‑only deploy key. We just gave that access key permissions to our repository. The owner is still root. Our new project shows up, juice‑shop, and let's save the job. It successfully connected to GitLab, but there is nothing to build. The repository is still empty. Let's head over to our console to add some content. Similar as to what we did with the tools‑image repository, we're going to switch the repository around. We pulled the code from GitHub, but now we're going to push the code to GitLab to our personal GitLab instance. Next, I've added a basic Jenkinsfile. This contains the pipeline configuration for the job. Let's take a look at that pipeline file. This pipeline file consists of two stages. The first one was demo earlier, the linting of the Dockerfile because this repository also contains a Dockerfile. The second stage is new. This one detects new secrets. Here we're making use of the tools‑image that we created with the previous job. There are quite a few parameters there. What important is to note is the baseline that is specified, and it only looks at the files that change between commits. We will not be diving deep into configuration of tools here during this course. I'm going to exit bat. Now I'm going to add the Jenkinsfile to our repository, as well as the detect‑secrets baseline that we created earlier. Now both files are staged. I'm going to commit the new files. Oops, let's try that again. I'm going to commit the new files, add Jenkins pipeline configuration. As you can see, the detect‑secrets hook fired, but we didn't add any secrets, so our commit passed. Now we're going to push our code, our new code, to the GitLab repository. And if everything went well, the code is pushed to the GitLab server. Jenkins got a signal, a webhook, that new code has been committed, and it will try to perform a build using the Jenkinsfile we just added. Let's take a look at the Jenkins web interface. Juice‑shop, the job is blue. I'm going to click on master, and there you see that our build succeeded. It linted the Dockerfile, and apparently it doesn't contain any issues. And it tried to detect new secrets, and we didn't add any secrets. So our build succeeded. But is it really working? Let's try to add a secret to the code base. Let's go back to the console. I'm going to create a new branch called secret. Again, I'm going to create a file containing a password, and I'm going to create the file this time secrets.txt. I'm going to stage the file. But now, when I try to commit, this will probably fail. This is as expected. We cannot add the secret to the repository, well at least not without adding it to the secrets baseline file and auditing the secret. How can you bypass a pre‑commit hook? That's actually very easy. You need to add a parameter to the git command. I'm going to repeat the last command and then at the flag ‑‑no‑verify. By using this parameter, you can bypass any pre‑commit hooks. Now we were able to add secrets to our code base. I'm going to clear the screen, and now let's push this branch to GitLab, and we're going to call the branch secret. We successfully added our secret to the code base through the secret branch and pushed that code to GitLab. Let's take a look at what Jenkins says. A new branch has been detected, secret. And what the red light immediately signifies the build failed. The detect new secrets stage failed because secrets were added to the code base without being verified first. Exactly what we wanted. Hopefully this showed you how easy can be to detect secrets in your code base. In the next clip, we're going to take a look at the workflow for detecting secrets.

Workflow and Conclusion of Detecting Secrets
The workflow for detecting secrets. First, generate a baseline, and the baseline should consist of a list of current secrets, as well as configuration settings for the tool that you're using. Audit the list of secrets. Check whether it's a false positive or a secret. This is a step we did using the detect‑secrets tool and the audit command. Add the baseline to the repository. For every build or scan. compare the results with the current baseline and warn or fail a build when detecting new secrets. This is what happened during the demo. A new secret was detected, and therefore the build failed. We could also have updated the baseline. In other words, should you have to add secrets to the code base, make sure to update the baseline. Detecting secrets and then, in particular, the detect‑secrets tool we used. It detects secrets in repositories, it has a highly pluggable architecture, we didn't dive into that part, and it's highly customizable. Advantages of using detecting secrets tools is quick wins, easy to implement, quick results. Another advantage is that it creates an overview of the current status, so you know where you stand. And having an overview of the current status makes it easy to gradually roll over to gradually remove all the secrets from your code base. Compatibility. Is it easy to use within existing toolings in your workflow? Most of the the tools understand Git. They're for also things like gitignore files. And almost all tools work with plain text files. Trialability. How easy is it to trial a tool, such as detect‑secrets? Most of them are command line‑driven and therefore easy to employ in continuous integration pipelines. They don't have much prerequisites. For more information, you can visit the truffleHog repository, the pre‑commit.com site for the pre‑commit framework, the detect‑secrets site, and the site for the second project we used, juice‑shop. In the next clip, we're going to take a look at the using code quality systems.

Using Code Quality Systems
We are getting more and more advanced in our quest to perform automated code security testing using code quality systems. What can a code quality metric system do? Well, the same as with linters, it can detect formatting or styling issues. It can also suggest best practices. So those two items could also be done by a linter. What else can a code quality metric system do? It can give you an objectified view of the state of the code. And with objectified, I mean opinionated by using agreed‑upon standards. It cannot only give you a current view of the state of the code, but also the state of the code over time, how it progresses or how it regresses over time. It makes the quality of your code visible. Most code quality metrics systems have a nice dashboard, and dashboards mean visual eye candy. It increases the overall quality of the code and thereby it makes maintenance of code easier. Needless to say, this all leads to more secure code, what we are working towards. There are also issues with code quality metric systems. The tools are often resource‑intensive and slow, and this is something to consider when using it in your continuous integration pipeline. The information overload can lead to focusing on unimportant issues, especially with dashboards and all kinds of metrics. Does it really help you, or is it just information overload? And all those metrics might give you a false sense of security or insecurity. It might look that your code is secure, but it's actually not or the other way around. Where and when can you use code quality systems? The systems are often located at the build phase, when developers push code to the repository and the build server picks up on that. Usually it triggers then external systems, which run code quality metric systems. The tool that will be demoed in the next clip is SonarQube, and this is an open source code quality metrics tool. In order to demonstrate SonarQube, we're slightly modifying our current demo lab. Right now we have GitLab, Jenkins, as well as the Docker registry service running on our demo lab server. We're going to add a fourth one, SonarQube. Again, this will be a Docker container, and we will be adding it to the docker‑compose file. SonarQube will be listening on port 9000, and we'll give it the name sonarqube.demo.local. Let's hop over for the next clip to start our demo.

Demo: Installing a Code Quality Metrics System
In this demo, we will be installing a code quality metrics system. We're going to run and configure SonarQube, and we'll do that using docker‑compose, and we will configure Jenkins so that it can communicate with SonarQube. Let's switch over. We're back at the console in the devsecops‑lab repository, which can be found on GitHub, I've checked out tag demo_7, and this docker‑compose file is basically the same as we used before to run GitLab, Jenkins, and Docker registry. We've only added a new service, SonarQube. The SonarQube image is called SonarQube with the tag 7.9‑community, and we're going to expose port 9000. We create several mapped volumes in order to store all the configuration files, and those volumes are defined below. I'm going to exit bat and clear the screen. And with the command docker‑compose up ‑d sonarqube, we now add the SonarQube service. It's now pulling the image and starting SonarQube at the background. The other containers, GitLab, Jenkins, and registry, are still running. We've merely added a new service, SonarQube. The next steps are configuring SonarQube and configuring Jenkins so that it can communicate with SonarQube. Let's switch over to the web interface. Let's go to sonarqube.demo.local port 9000, which we mapped to the SonarQube container. And SonarQube seems to be up and running. Now let's do some configuration. I click on the Log in button, and I'll be using the default username/password, and that is admin, admin. I'm going to click under top‑right A letter from Administrator and then select my account. Then I'm going to select Security. I'd like to do two things here. The first one is I'd like to generate a token, which we'll need for Jenkins to communicate with SonarQube. And, of course, I'd like to change the default password into a stronger one. First, the token. I'm going to name the token Jenkins, click on Generate, and I'm going to copy the token, and I'm going to change the default password admin to a strong password. Even though this is a demo environment, it's always good practice to use strong passwords. I click on Administration and then on Marketplace. I want to make sure that all plugins are up to date. So I'm going to select Plugins, Updates Only, and I'm going to update all plugins. This is just good practice, making sure that the system is up to date. This is a security course after all. It asks for a restart of the server. I'm going to restart the server, and then I'm going to switch to Jenkins to configure communication with SonarQube. I'm going to select Manage Jenkins and then Manage Plugins. As we want to install a SonarQube plugin, I'm going to select the tab Available and search for SonarQube. I'm going to select the plugin SonarQube Scanner and then download now and install after restart. Select Restart Jenkins, and this now will install our SonarQube plugin. The last thing we need to do right now is enter the token we generated earlier on to the SonarQube plugin. Jenkins has restarted. I'm going to Manage Jenkins and select Configure System. Then I'm going to scroll down a bit until I see SonarQube servers. This is a new item added by the plugin. I'm going to select Enable injection of SonarQube server configuration, and I'm going to add a SonarQube installation. Let's name this SonarQube installation sonarqube.demo.local, and the server can be reached at http://sonarqube.demo.local port 9000. Then we're going to add the server authentication token that we just generated by SonarQube. Click on Add server authentication token. And here we're going to paste the token that we just generated. And the type is secret text, the secret is the token value, and the ID is SONAR_AUTH_TOKEN in all caps. I'm going to use the same value for description. Click on Add. Now select the token we just generated, SONAR_AUTH_TOKEN, and click on Save. Now we've configured Jenkins to talk with SonarQube. In the next demo, we're going to use SonarQube in an existing pipeline.

Demo: Analyzing Code during Automated Builds Using SonarQube
We just configured SonarQube, as well as Jenkins. How about tying those two together? In this demo, we're going to use the code quality metric system, SonarQube to be exact, and use it in a Jenkins continuous integration/continuous deployment pipeline. We will be doing this for our OWASP Juice Shop project. I'm back at the console in the juice‑shop repository. I'm going to create a new branch, and I'm going to call the branch sonarscanner. And in this branch, I'm going to configure the Jenkinsfile in order to automatically perform a code quality metrics tool scan, in other words sonarscanner, using SonarQube. Let's open up the Jenkinsfile. I'm going to update the description, and I'm going to add a new environment variable called SONAR_KEY, and we will be using this variable in order to define the project or use the project name for SonarQube. Let's now scroll down and add a new stage where we perform the sonarscanner step. I'm going to name the stage sonarscanner, and I'm going to use the same Docker image that we created before, the tools‑image. So the agent configuration is just a copy of the configuration we use before with the detect new secret stage. So we can run the sonarscanner, the binary which scans the source and pushes the results to the SonarQube instance from the Docker container itself. Then I'm going to add two steps in this pipeline. One is installing a prerequisite for the sonarscanner that's installing typescript, and the second one is actually performing the scan by running the sonarscanner binary, which we installed on the tools‑image over to the source code, and it will send those results back to the SonarQube scanner. The withSonarQubeEnv is a Groovy statement provided by the SonarQube plugin, and the goal of the demo is not diving too deep into the exact working of the plugin, but merely to show you how you can perform such a test in a pipeline. I'm going to save this file and exit. I'm going to clear the screen. Now let's stage our modified Jenkinsfile and commit it to the repository. As you can see, the pre‑commit hook still fires. And no, I haven't added any secrets into the Jenkinsfile. And we're now going to push the code to our GitLab server to the branch sonarscanner. Let's hop back over to Jenkins to see whether it picked up on our new branch and whether it started scanning in the background. If everything went well, we should see a build starting at the left‑hand side. And yes, we see a new build starting. Jenkins picked up on the new branch sonarscanner and started building this branch. I'm going to click on the branch to get more information about that build steps. Here you can see it checks out the source code. It lints the Dockerfile. Well, we haven't changed the Dockerfile, so that should be okay, Then it will check for secrets. We haven't added any secrets. And then, it will perform our new step, sonarscanner. As we said before, this might take a while this step. What you don't want is that automated security testing tools hold up the whole process. So you could choose to perform this scanner asynchronous, for instance every night. You could also perform the scan, as we're doing right now, for every change or every commit to the branch. The luxury of watching a Pluralsight course is that you can actually speed up this step. So we're going to speed up this step, and we're going to switch over to SonarQube to see what SonarQube has to say about these results. And here you can see that it analyzed the juice‑shop project. The juice‑shop name it actually derives from the SONAR_KEY variable that we set in the pipeline. If you click on it, we see loads of metrics, a beautiful dashboard. We see a metric on reliability, a metric on security, a metric on maintainability, and so on and so forth. And this is what we were looking for. This makes the quality of the code visible. It gives an objectified view of the state of the code. I'm going to click on Security Hotspots. Let's dive a little bit deeper into some of the items that SonarQube has detected. And here you can see all items that sonarscanner detected as being a potential security issue. We can drill‑down on such an issue. Let's take that first one, and here it alerts about hashed data, whether it's secure or not. You can drill‑down by clicking on See Rule and then more detailed information about the potential security vulnerabilities presented. And this might be very useful information for developers. SonarQube can give loads of more Information about your code This is just a brief demo in order to show you how you can set up a code quality metrics system. In the next clip, we're going to take a look at the workflow and conclusions of using such a tool.

Workflow and Conclusion of Code Quality Metrics Systems
A workflow for using code quality metric systems. Select a system with sufficient support for your applications, language, and frameworks. In our example, ample support for mainly JavaScript was available. We immediately saw results without having to install additional plugins. Then, let the system generate a list of current issues it finds. And, very important, audit that list of issues. For instance, is it a false positive or is it actually an issue? We didn't dive deeper into that during the demo, but it might very well be that numerous issues were actually false positives. And if it's a valid issue, should it actually be shown? You want to reduce the chance on information overload. And even though the dashboard was nicely filled with metrics, do those metrics actually matter? Therefore, make sure that you configure the rules. Tweak the rules to what makes sense to your specific setting. And when everything's correctly set up, compare every scan result with the previous results so you see differences over time, regressions, as well as improvements. A code quality metric system. We saw that it makes quality of code visible, it gives an objective view of the state of the code, and it will suggest best practices. The advantage of using such a tool, well, the graphical dashboard was really handy. We immediately so some results on the quality of the code. It can also give insight into the impact of changes. As soon as new code is committed and subsequently scanned, you can see the results of that displayed on the dashboard. Compatibility. Is it compatible with your tools and frameworks? Well, that depends on the language. SonarQube, for instance, has lots of plugins and lots of support for numerous languages. Trialability. Is it easy to start using such a system? The setup was moderately easy as you hopefully saw during one of the demos. However, configuring and correctly interpreting the results can be very time‑consuming, so you really should plan out time in order to do something with the information that you will be shown. More information can be found on the SonarQube website, and the used Jenkinsfile can be found in the corresponding repository of this course on the devsecops‑lab repository. And this is a perfect moment in the course to say do not underestimate the time it takes to properly configure security testing tools. This really can take up a lot of time. Therefore, plan accordingly. In the next clip, we'll do a brief summary of this whole module.

Module Summary
We dealt with quite a few tools in this module. We saw linting. Linting can give you quick feedback on the status of your code. What's important to remember is to use strict versioning for linters. When you use tools, make sure to enforce specific versions for your project so that everybody uses the same linting rules. Detecting secrets. Hopefully, you saw that it's very easy to implement, and therefore it's a quick win. There is not a good reason not to start using detecting secret stools immediately. Code quality metric systems is also something we discussed in this module. It can show advanced reporting metrics. The dashboard looked shiny, and there were a lot of metrics to look at. However, it can be time‑consuming to configure and therefore to use. So really think carefully before starting to use it whether it makes sense in your situation. Some tools were easier to use than expected, says Maeve. Well, great to hear that, says Jennifer, our consultant who helps. Are you also interested in automating third party libraries security testing? Absolutely, says Maeve. Let's go. In other words, that's what we're going to talk about in our next module, automating third party libraries security testing.

Automating Third Party Libraries Security Testing
Introduction
Welcome to the module Automating Third‑party Libraries Security Testing. Let's go back to our scenario with Maeve working for Globomantics and consultant Jennifer. Maeve says, now we looked at our code. This is what we did in the previous module. But should we also look at third party code that we use instead of only at our code? Yes, absolutely, says Jennifer. And let me show you how you can do that. In this module, we will be taking a look at third‑party libraries scanners. We're going to take a look at where and when to use such a scanner, and we have two demos. We're going to scan for outdated and insecure third‑party libraries on the command line, as well as integrate that scanner into our pipeline. And we try to make it flag built when there are actually any insecure third‑party libraries. And, of course, we will be finishing off the module with a summary. Let's briefly zoom out again. In the automated security testing realm, we are performing automated security testing, and this module deals with third‑party libraries. And within this module, we'll mainly be looking at scanning third‑party libraries. In the next clip, we will be taking a look at third‑party library scanners.

Third Party Libraries Scanners
We barely started this module, and already I'm plugging another Pluralsight course. The course Secure Coding Using Components with Known Vulnerabilities gives you much more in‑depth information about third‑party libraries scanning. So if you want to learn much more about this specific subject, I recommend to watch that course. Before performing such a test, we will be going over the basics. The security of an application degrades over time. Where, say, 20 years ago, the MD5 hashing algorithm was considered secure, now it is not. It is considered broken. Attacks on software become cheaper and faster. Not only that, but the security of your application depends on other parts. You can have the best software development team. Chances are that they will be using third‑party libraries. And are those third‑party library secure? If such a library contains a vulnerability, then your application is insecure. So the security changes irrespective of changes in the code itself. The fact that a code base doesn't change doesn't mean that the security doesn't degrade. Therefore, it is very important to regularly test for outdated and insecure third‑party libraries. How do third‑party library scanners work? Most scanners first download lists with vulnerabilities. These lists are published by vendors and other centralized organizations who compile those lists. Then, in a separate process, the scanner tries to fingerprint all the libraries that are in use, and it generates a list of all those components. The scanner itself tries to match the components that are use with components that have known vulnerabilities. And if it finds a match, then it will alert you. Characteristics of a good scanner are that it uses multiple up‑to‑date sources for vulnerability reports. If the lists are incomplete, then it will not find any vulnerabilities. Then it might not find the vulnerability in the third‑party library. A scanner is also dependent on finding those third‑party libraries when it fingerprints them. It needs to understand multiple frameworks in order to detect versions. And what can help scanners is when it can parse manifests or software bill of materials. Most frameworks nowadays work with manifests, which specifies exactly which versions of which libraries are in use. And if a scanner can parse that information, then it has the best detailed information available, which will help with matching a vulnerability against it. So what can a scanner for outdated libraries do? It generates an overview of all the components that are in use, including detailed version numbers. Then, it generates alerts for the outdated and/or insecure libraries. And one feature of such a scanner is that it can serve as an asynchronous quality gate. And what we mean with asynchronous here is that you can perform the scanning regardless of changes in the code base. Actually, sometimes it makes more sense to not scan when people commit code, but only scan on a regular base. We will look at that characteristic later on. Where can you deploy or use such scanners? You can use them in the build phase when the automation or build server retrieves code from the repository, tries to fingerprint it, and then match it against lists of known vulnerabilities. You can also use it when pushing artifacts to a registry or to an artifact server. When storing those artifacts, then you can perform scans. This is something which we will demonstrate later on. Actually, another phase is the pre‑commit phase. This is relatively new, but you do have integrated development environments who already look whether you're using outdated third‑party libraries or not and prevent you from committing that code. And then back to the asynchronous part. Scans usually take a long time. Some scans are very intelligent with regard to creating fingerprints, but therefore also very complex. And the results, as we mentioned, vary independently of the changes in code. Therefore, it is recommended to perform periodic scans. And why? Well, the scans during builds immediately after commits might muddy results. Say a developer makes a change in the code base, then he checks in the code, and the scanner detects an issue with regards to a third‑party library. That has nothing to do with the change that the developer just did. And in order to make it clear that a build fails because of a third‑party vulnerability, it makes more sense to perform scans asynchronous. The tool that we will be demoing in the next two clips will be OWASP Dependency‑Check, another open source project. This tool will attempt to detect published vulnerabilities in used libraries. See it in action in the next clip.

Demo: Using OWASP Dependency Check from the Command Line
In the first demo of this module, we will be using OWASP Dependency‑Check in order to find vulnerable third‑party libraries. We will be using Dependency‑Check on the command line as a Docker container. I'm back at our server, and I'm in the juice‑shop directory where we cloned the juice‑shop repository. So let's say I want to scan this code base for outdated third‑party libraries. I want to run the Dependency‑Check binary. I'm going to run it as a Docker container. So I'm going to specify the docker run command, and I'm going to use backslashes so that you better can see all the parameters that we're specifying. I'm going to specify the specific user to run this container under. Why? Well, unfortunately, the Dependency‑Check container is a bit finicky with regards to user permissions, and I wanted to be able to write a report to this current directory. I'm going to specify the rm flag. I don't want to container to be around when it's finished. Then, I'm going to map a named volume, and I'm going to call it dependency‑check, and I'm going to map it to the container where it can cache all downloaded vulnerability reports. This will speed up the scanning process after subsequent scans. Then I'm going to map the current directory to /src directory. This is a source that I want to scan. I'm going to specify the name of the container, owasp/dependency‑check, and I'm going to use a specific version. Unfortunately, Dependency‑Check isn't stable every time, and from this version number I know it will perfectly work. So that's why I'm specifying 5.3.0. I want it to write out a report in the /src directory, and I want it to scan that /src directory. Let's execute the command. First, it will download the image. And after it's ready downloading the image, then it will start collecting all the lists of vulnerabilities. So this might take a while for the initial scan. It's now downloading all those files to the named volume that we call dependency‑check, and this will be faster for subsequent runs. After it finished downloading all the vulnerability reports, then it tries to fingerprint all the third‑party libraries that are in use. It warns about not being able to find installed Node packages, but we'll leave that for now. But right now, let's look at the report that it generated. So as you can see, it's fingerprinted a lot of files, but no files with known vulnerabilities. This is all bespoke code and doesn't contain any known or published vulnerabilities. Of course, that can change. You can expand on the report and look at the fingerprint information. But is it working? Let's do something different. Let's download a vulnerable version of jQuery using the curl command, and let's download version 1.9.0, which is an old version that you shouldn't use, and let's download it to this directory. Now we know that this repository, or actually this directory, contains a vulnerable third‑party library. Let's run the scan again. Now it says something different. Let's go back to the HTML report and refresh the page. And here you can see that it correctly identified our jQuery version. It flagged it as being vulnerable, and it shows various information about the vulnerabilities that it contains. Demo succeeded. In the next demo, we'll be integrating this in our pipeline.

Demo: Using OWASP Dependency Check in a Pipeline
In this second OWASP Dependency‑Check demo, we will be integrating Dependency‑Check in our build pipeline, and we will be using the same Juice Shop project. Let's create a new branch called dependency‑check, and let's edit the Jenkinsfile, the configuration of the pipeline. Now let's add a new stage, which performs dependency check on the source. Let's add a quick description of the new stage that we're going to add, and then I'm going to scroll down, and I'm going to add a new stage. I'm going to paste this code in and call the stage dependency‑check. We want to run dependency‑check from the Docker image, the same one we used in the previous demo. The settings used are almost the same. The only thing changed is that we map a new folder, reports, where to write that report to. That way the source stays read‑only, and we can write the report to a different directory. We actually don't want a third‑party container to be able to touch our code. And this scanner step is also almost the same as in the previous demo. The only difference is that we let the build fail when it finds any vulnerability having a CVSS score, a metric of impact of the vulnerability, of 6 or higher. And the last modification that we will do is that we publish, show in Jenkins, the generated HTML report so we can view it from within Jenkins. By the way, there are also Jenkins plugins, which do this automatically. But this course is about showing you how to perform tests, not specifically doing things for Jenkins or use specific plugins. That's why we're not using those. And this step publishes the generated HTML report. I'm going to save the file, exit Emacs, clear the screen, and now I'm going to add and commit the Jenkinsfile. I am going to push the code to our GitLab server, and let's look at Jenkins at the results. As you can see, it started building the dependency‑check branch. I'm going to click on the branch, and here it is performing all pipeline steps. As discussed earlier, it isn't a good idea to perform these steps for each code commit. It's better to perform these scans asynchronously. However, for demonstration purposes, we run all these tests for each code commit. But scheduling these types of jobs is better. It now runs dependency‑check. And thanks to speeding up the video, we can immediately see the results. It built successfully. Or, to be more precise, the automated security test succeeded. As you can see, we have a new menu item at the left, Dependency Check Report. Let's click on it. And here you can see the results of our scan, the same as we saw in the previous demo. No vulnerabilities have been found. But now let's make it a little bit more interesting. Let's go back to the console, and how about adding the insecure jQuery library? We're adding that jQuery file, committing it, and pushing it to the repository. Now let's switch back over to Jenkins and see whether the build now fails. It picked up the change, it's running the build, and, indeed, the build is marked as failed, exactly as we hoped it would do. It now detected that the repository contains an insecure third‑party library, and therefore the build is marked as failed. The security test failed. If you hover over the stage, you see our error message, Insecure libraries found. And if you now click on the Dependency Check Report, you see that it successfully fingerprinted our jQuery library and flagged it as being insecure. Pretty neat. In the next clip, let's look at the workflow of using third‑party library scanners.

Workflow, Conclusion, and Summary
The workflow for third‑party libraries scanners. Schedule a scanner. Even though in the demo, we did it synchronously, the scan was run as soon as we pushed a build to the Git server. However, it's better practice to schedule the scanner, say, for instance, once a week or once a day. Don't forget to regularly audit the list off libraries. Does the scanner still detect all third‑party libraries that are in use, or does it skip some? And ensure that when it finds a vulnerability that alerts are generated that you will be notified about a failing build or actually a failed security test. And, of course, with all security testing tools, plan to perform updates when it is necessary. So the third‑party library scanner. It alerts you about insecure and outdated libraries, and it generates an overview of all the libraries that are in use. A big advantage is that you will be actively alerted about insecure and outdated libraries. You only have to configure it once. Compatibility. Well, that depends on your framework and language. Major languages and frameworks are supported. You're also dependent on the list of vulnerability reports. If a supplier doesn't notify everybody that a security vulnerability has been found, then you won't be notified. Trialability. As you might have seen, it was moderately easy to employ it in our continuous integration pipeline. However, the follow‑up can be time‑consuming. In our example, not many or actually no third‑party libraries were found with the scanner configuration we used. But in a usual setting, this can be really time‑consuming. However, there are quite a few commercial offerings that do this automatically for you. They will create pull requests based on libraries that need to be updated. More information can be found on this site of OWASP Dependency‑Check. To summarize this module, security degrades over time. It's always good practice to run asynchronous scheduled scans, and plan in time for updates and upgrades. Next up, Maeve says, that was very useful. However. I wonder, though, about the security of our containers. That's a great observation, says Jennifer. Let's test those in the next module. Coming up, automating container security testing.

Automating Container Security Testing
Introduction
Welcome to the module Automating Container Security Testing. Let's get back to our scenario with Maeve, who wants to implement automated security testing, and consultant Jennifer. Maeve asks, are all of the containers we're using actually secure enough? Are they outdated? Do they contain known vulnerabilities? Do they comply with our policy? Let's take a look at all those containers we're now using, Jennifer says, well, I'll demonstrate a container security scanner for you. Container security scanning is relatively new. However, there are already several good solutions on the market. Looking at container security scanning, actually performing a container security scan, is what we're going to do in this module. Let's grab our overview again from Automated Security Testing. We're currently performing, and we're looking at containers. Within containers, we're going to look at container scanning. This module will consist of container security scanning, and we'll have two great demos. One will be scanning and validating a container from the command line, and another demo will be scanning and validating a container in a build pipeline. Slowly, but surely, we'll be working towards a grand finale with of our demos. Of course, we will be finishing off the module with a summary. Let's get started with taking a closer look at what container security scanning is.

Container Security Scanning
Container Security Scanning. What can container security scanning do? What can it do for you? Well, this might sound obvious, but it can detect insecure containers, and it does so by detecting, for instance, outdated libraries. Not only that, it can also detect incorrectly configured containers and even detect an outdated operating system or an outdated base image, all of the items that make up an insecure container. It can also be used to detect compliance validations. For instance, you can set up a policy, which your container should adhere to, and then check whether it actually complies to that policy. And similar to linters, it can suggest best practices. How does container security scanning work? It's a little bit the same as we saw earlier with scanning for outdated or insecure third‑party libraries. A scanning system downloads lists with known vulnerabilities in operating systems, as well as third‑party libraries. It compiles a list with all those vulnerabilities. Then you can add a container to scanning queue. The container is analyzed, actually all layers of the container are analyzed, and the scanner fingerprints all libraries. Then most scanners actually check also other items of the system, for instance permissions or open ports, even health checks. Then, those lists will be cross‑checked with the list of vulnerabilities, as well as the required policy. And if there's a match, or if there's a non‑match, you'll get an alert. What are the issues with container security scanners? The level of depth really depends on the tool you're using. As our consultant said in the beginning, this is a relatively new field, and the results that you'll get are very dependent on the type of solution you choose. It's also easy to go too far with configuration. Going too far? What do we mean by going too far? Well, there are tools where you can configure so much different settings, that it's easy to jump overboard. You really to need to take a look at exactly what you're trying to configure, what you're trying to achieve. In other words, the scan results, will it lead to actionable events? Are there actually things you're going to change depending on the scanner results? If you're using, for instance, base images and you're not willing to change those images, then you should ask yourself, should I actually be using a container security scanner solution? Where and when can you use container scanners? You can use it at the build phase when you're actually building, for instance, a Dockerfile and looking at the resulting image that you're creating. Another location to perform container scanning would be when you push a container to the registry or when you pull a container from a registry. The tool that we will be demoing in this module will be Anchore Engine. This is a Docker container analysis and compliance tool and, as with all tools that we're demoing in this course, open source. We have two separate demos ready for you in the next clip. Let's start with the first demo.

Demo: Performing Container Security Testing on the Command Line
In the first demo of this module, we are going to scan and validate a container. And for that purpose, we're going to configure and use Anchore Engine on the command line. In order to run the Anchore Engine framework, we're going to make use of another docker‑compose file. I'm now in the devsecops‑lab repository, and I'm going to the anchore‑engine directory. This directory also contains a docker‑compose file, and let's take a look at it. This docker‑compose file defines several services. All of these services are necessary in order to perform our container scanning. There's an API service to interact with the engine. There's a catalog service and state manager of the system. There's a queue for the queuing system. There's a policy‑engine to take care of validating policies of containers. There's an analyzer service, which takes care of analyzing containers. And there's a database service, Anchore Engine's persistence layer. So this is a rather complex docker‑compose file. The Anchore Engine framework consists of six different services. As you can imagine, the infrastructure requirements for this engine are pretty high. Now let's exit bat, clear the screen, and perform a docker‑compose up. This will start all defined services and therefore the Anchore Engine. I'm going to specify the detach flag in order to let the services run in the background. First, it will download all images. The reason we're using another directory, anchore‑engine, is that docker‑compose expects one directory to be used for one docker‑compose file. And now after downloading all the images, it will start all services necessary for performing our first container scan. We're going to clear the screen. We can interact with the Anchore Engine using a command line interface, and there are two different ways we can invoke a command line binary. The first is executing the binary directly on the Docker container that is running using docker exit command. I'm adding the name of the container, the name of the command line interface, anchor‑cli, and the commands that I would like to execute our system status. This will give us an overview of the running system. As expected, or as hoped, we can see that the service is up and running. All six services are up and running. The second way to interact with the command line interface is by running our tools‑image, the image that we built in an earlier demo. For this, I'm going to execute the docker run command with the rm flag, then the name of our image, tools‑image, and then the same parameters as we just used, anchore‑cli and system status. Hey, this doesn't work. Why doesn't this work? Well, as you might remember, all of our services that we've defined so far, GitLab, Jenkins, the registry server, as well as Anchore Engine, all used the same named network, and the network is named lab. When I now run a new container, it will not start in the same network and therefore not be able to communicate with Anchore Engine. So if I now pass the network flag to the docker run command and specify the name of the network, lab, then it should work. And indeed it works. We get the same results back as we got went running the binary directly from the running docker container. The second technique, running the binary from our tools‑image container, is a technique we will be using when implementing container scanning in our pipeline. The command line interface, anchore‑cli, gives us an opportunity to interact with the engine. By providing the command system feeds list, we see all of the vulnerability lists that the engine has downloaded so far. This is a continuous background process of Anchore Engine, continuously downloading the latest lists of vulnerabilities for several operating systems, as well as third‑party libraries. But let's start scanning a container. By performing the image add command and then with the name of the container, we can add a container to waiting queue, and then it will be analyzed and graded. Now we can see that the image has been added to the analysis queue. The analysis status is not analyzed, which makes sense because we just pushed the image to the queue. As we are impatient, immediately, let's try to get results. We can do that with the command image vuln. That looks as if the results are already available. So let's try to get some details about the vulnerability reports. We can do that by adding the parameter all after the command. Now we see that the image is actually still not analyzed, which makes sense. The analysis status is still analyzing. So how can we wait for the scanning process to complete? Well fortunately, Anchore Engine also provides a waiting thread. That's something you can do with that wait command. So I'm going to repeat the last command, arrow up, and I'm going to replace vuln with wait, and now the command is locked and will be released as soon as the image has been completely scanned. This command provides you a convenient way to implement container scanning into your pipeline, and the scanning of containers can take up quite some time, depending on the complexity of the container that you're scanning and also of the underlying infrastructure that you have the Anchore Engine running on. The scanning has completed. The analysis status is now analyzed, so we can clear the screen and repeat the previous command in which we looked at the vulnerabilities that were detected. And here you get an overview of all vulnerabilities. Apparently our new Docker container, tools‑image, doesn't contain any vulnerabilities, which makes sense because we just built the image. And fortunately, the base image that we used to build the tools‑image is apparently also vulnerability‑free. In the next demo, we will be implementing Anchore Engine scanning into our pipeline, and then we'll be using a little bit more complex example.

Demo: Implementing Container Security Testing in a Pipeline
In this demo, we will be scanning and validating an image, and we will be doing that in a pipeline, to be precise in the pipeline of the Juice Shop project. We're going to add a build image stage to the pipeline, a push stage to the pipeline, and a scanner stage to the pipeline, and we'd like to fail the build when the policy check of the scanner fails. We are in the juice‑shop directory, and let's create a new branch, and let's call it container‑scanning. Now let's open up the Jenkinsfile, but not with the Emacs editor, but this time with bat because secretly in the background, I already have been modifying the Jenkinsfile. One of the advantages of using bat to show source code is not only that it gives you syntax highlighting, at the left‑hand side, you also see the Git integration. You can see the changes with regards to the previous version. That makes it easier to follow along. On the top, I've added a new variable called DOCKER_IMAGE. This variable contains the name of the image that we're going to push to the registry server. Let's scroll down for the first new step, build image. The important part here is the docker build command. It reads the Dockerfile from the repository and tries to build a Dockerfile. Depending on whether the commit is tagged and the branch name, it will add a tag to the Docker image name. Let's scroll to the next new step, and that is push to registry. This pushes the freshly built image to our registry server. And then the most important new stage, scan container. So this runs our Docker image, the TOOLS_IMAGE, it makes sure it's connected to the lab network, and it performs the commands we just did in the previous demo except for one difference. First, we checked the status of the system. Then, we pushed the freshly built image to the scanning queue. Then, we're going to wait for the analysis to finish. And then, we're going to generate a list of vulnerabilities and add that to the file anchor‑results.txt. So this is the same as we did in the previous demo. In the new step, evaluate check, we're evaluating a policy against our DOCKER_IMAGE. And if the policy check failed, then we will fail the stage. Against which policy you might think? Well, during the build phase, if everything goes well, we shall take a look at all the policy files. Let's clear the screen, add the Jenkinsfile to the repository, commit the file, and push the file to our GitLab server. Now quickly hop over to the Jenkins server to see whether it picked up the branch and started the build process. Our workflow still works. It detected the new branch container scanning and started building right away. Let's take a closer look at the build steps. After linting the Dockerfile, it starts building the image, just as expected. This is going to take a while, so let's hop back over to our console and look a bit closer to the policies. With the command policy list, you can get an overview of which policy is currently active. This is the full policy with Anchore Engine. Using the policy get command with the policy ID, we get a little bit more basic information about the policy. But that's not enough for us. What exactly is this policy? What is our image currently being checked against? Executing the same command, but then applying the parameter ‑‑detail, we get some more information about the policy. I know this is going to be in JSON format, so I'm going to pipe it through bat and specify that there's a JSON file to get some nice syntax highlighting. That makes it easier to view the JSON file. And immediately, you see a blacklisted_images list variable that looks that you can blacklist images. Then there are some other policies you can set, for instance that apparently the policy fails when port 22 is exposed. It's not my intention to go too deep into the policy to show you exactly what means what, but more to let you see how fine‑grained a policy can become, what you can do with it. It's not just a check on an outdated operating system or outdated third‑party libraries, but much, much more that you can do with Anchore Engine. Let's go back to Jenkins to check up on the status of our build. It has been pushed to the registry and is currently being scanned. Ah, the scan has completed, and apparently it matches the policy. All of our automated security tests succeeded. By clicking on the little download symbol, we can get a closer look at the Anchore results, which is a build artifact. And if you take a look at the Anchore results, you see that it passed our policy. This is a relatively easy way to scan your containers, not only for outdated operating system files or outdated third‑party libraries, but also whether it matches or fails a policy that you set up. Let's take a look at the workflow in the next clip.

Workflow, Conclusion and Summary
The workflow and summary of container security scanning. The standard workflow. It's very important to agree upon what exactly you need to test. You can test a lot of things, whether the operating system is outdated, whether the container contains insecure third‑party libraries, file system permissions, open ports, health checks, and so forth and so on. Two important questions you should ask yourself before implementing container security scanning are can you do something with the output, and will you do something with the output? Especially in the field of container security scanning, it's easy to go overboard. My recommendation is use the policies sparingly. Does it make sense to go in such detail with regards to configuring policies for containers, especially when you will not do anything with the output. And shouldn't containers be immutable anyway? When a container is flagged as being insecure, well, of course, then you should modify, update, or upgrade the container, also known as rebuilding the container. Container scanners. They can detect compliance validations when checking against policies, they can detect out‑of‑date libraries and out‑of‑date operating systems, and they can suggest best practices. The advantage of using container scanners are that it hardens containers. It also ensures that an out‑of‑date operating system is being noticed. Compatibility. How well does it fit within your current toolset and within your current workflow? Docker's open container initiative image format is widely supported, and that's basically the default standard for using containers right now. Trialability. How easy is it to try out a container scanning solution? Well, it needs substantial infrastructure requirements. It's not something you just plug on a test server. What's also a factor in trialability is the rapidly changing landscape, the choice for a framework. Container security scanning is fairly young, and the landscape changes rapidly. Be aware of that before implementing a container scanning solution. More information about the Anchore Engine that was demoed in this module can be found on the anchore.com website /opensource. To summarize this module, container security scanning can be handy. You can be notified when containers are outdated or insecure. What is a fact is that container security scanning becomes more and more important. More and more companies are using containers. However, it's relatively more difficult to implement. From all the tools we saw so far in the course, this is one of the harder ones to implement in such a way that actually makes sense for you. Next up, Maeve is not really convinced with container security scanning. She says, we will take a look at this in due time. Jennifer comes up with something else. So far, we focused mainly on static security scanning. Are you also interested in dynamic infrastructure security testing? Absolutely. Maeve is convinced. Let's go. Automating infrastructure security testing in the next module.

Automating Infrastructure Security Testing
Introduction
Welcome or welcome back in the last module in the course Performing DevSecOps Automated Security Testing. You're watching the last module in the course, Automating Infrastructure Security Testing. Let's hop over to our scenario. Maeve is happy. Finally, the application is ready for production. We've tested the code, we've tested for outdated third‑party libraries, we've tested for insecure containers. Have we tested everything? Jennifer responds, almost because we still need to test the infrastructure. But fortunately, we can also automate that. We will start off this module with infrastructure scanning, and then we have several cool demos. First, we will be scanning for web server misconfigurations. Then, we're going to perform dynamic application security testing using the OWASP ZAP scanner. And then we will be implementing all automated security tests that we have seen so far from the previous modules. We will finish off this module with a module, as well as a course summary. But first, let's take a look at where we are. This is the Performing DevSecOps Automated Security Testing course. And so far, we've looked at code, third‑party libraries, and containers. We're currently looking at the infrastructure, and, in particular, we're going to take a look at infrastructure scanning. That's what we will start this module with. In the next clip, we will take a look at that, infrastructure scanning.

Infrastructure Scanning
Infrastructure Scanning. What exactly can an infrastructure scanner do? First of all, it can find known misconfigurations. An example of a misconfiguration would be where a certain directory of a web server was open for directory browsing, and everybody could download all the files that were located in that directory. An infrastructure scanner can also find issues with missing hardening. An example here will be that detailed error messages are shown when an application enters a faulty state. Another thing that an infrastructure scanner can do is that it can detect known or published vulnerabilities. Scanners fingerprint the used software, like a web server header or the framework that's being used, and compare that with a list of insecure versions. Characteristics of good scanners are that it uses extensive testing methods, tests, for instance, for all open ports or tests for directory names or tests for function names. You want to test as exhaustive as possible. It should also be easy to suppress false positives. If you use extensive testing methods, you will run into a lot of false positives. That's just a byproduct of testing so much. Therefore, you want a scanner that allows you to easily suppress those false positives and not continuously be confronted with false alerts. Ideally, you would like a scanner that focuses on quality. Speed is not so much an issue because the scans, as we shall see later on, are usually asynchronous, run in the background. You want to have a scanner that focuses on real quality, looking at the application and the infrastructure versus a fast scanner. Where and when should you use infrastructure scanners and dynamic application scanners? Well, you'd like to do it as soon as the application is built, so when a release artifact is ready for deployment. And the release artifact can be an application, it can be a container, but it can also be a complete infrastructure. Another phase where you can perform scanning is during the deploy phase or actually immediately after the deploy phase when the application or container or infrastructure is actually running, which brings us to an interesting question and a question I personally, as a penetration tester, get asked a lot. Which infrastructure should you actually test for security defects? Usually one has a development, a test, and acceptance, and a production environment. As we discussed earlier in this course, and actually earlier in the DevSecOps path, the tendency is to shift security left. The earlier you can perform security testing, the better. So you'd rather test the development environment than say the production environment. However, you want to test the environment, the application, the infrastructure that's closest to what's actually in production. And unfortunately, that's usually production itself. If you can mimic production completely on, say, acceptance or even earlier environments, then by all means test there as long as you test an environment that resembles exactly what's running on production. And in practice, that means usually testing in production. So far, we've used the following workflow in order to perform tests. We have our build server or automation server, Jenkins. When performing a build, Jenkins pulls code from our repository from GitLab. Then, it can download or pull artifacts, in our instance, it pulls a container, and it performs the build process. After the artifact has been created, it can push those artifacts back to another server, a repository manager, or, for instance, our registry server. When the artifact is ready to be tested, we used Docker containers. For instance, for linting, we used a linter container. And for detecting secrets, we used our tools‑image container. We performed static security testing. But what now when you try to test a dynamic running application? Let me introduce you to the sidecar testing pattern. When the application is ready to be tested, we use another container to spin up the application. After the application has been spun up, then it can be scanned by other containers, such as the server scanner or the application scanner. After those scans have been completed, the dynamic scans, then we'll break down our application, or rather we shut down the sidecar running our application. The tools that will be demoed in this module. We're going to demo Nikto, which is an open source comprehensive web server scanner, and we're going to demo OWASP ZAP. OWASP ZAP is the world's most widely used web application scanner. It's also open source and another OWASP project. It is very convenient to use in a continuous integration environment, and we shall use this tool in our pipeline to perform an automated security scan. In the next clip, we'll be starting our demo with Nikto.

Demo: Running Nikto and Using the Sidecar Testing Pattern
In this demo, we will be using Nikto, the open source web server scanner. First, we will be using Nikto on the command line, and we will be using our earlier built tools‑image for that. Then we're going to add the sidecar to the pipeline, which we so discussed in the previous clip. And then we're going to scan a running container in our pipeline. We want to test our previously built Juice Shop Docker container. In order to do that, we're going to run a sidecar. We're going to do a docker run. We're going to name the sidecar sidecar. We're going to connect it to the network lab. We're going to map the port 3000. This will be for demonstration purposes. And we're going to specify the name of the image, which, in our case, is juice‑shop. So by performing this command, we will spin up a sidecar, which we then can test. The sidecar has been started, so now let's perform our Nikto scan. And to perform our Nikto scan, we're going to use our previously built tools‑image. We're going to run the container connecting to the same network, and we're going to execute the command nikto.pl, the name of the Nikto binary or executable. We're going to specify the hostname, which, in our case, is now a sidecar. And by default, the juice‑shop listens on port 3000. So by performing this command, we're actually testing the sidecar. The scan starts, and immediately you will be seeing results, for instance that the X‑XSS HTTP protection header is not defined. This is an example of a security misconfiguration and something we can fix. If this were actually an issue because it could very well be that all the notifications you now see scrolling on the screen are also false positives. So from each of those notifications, and, as you can see, there are a lot of them, you should first check whether it's a false positive or not because after using such a scan, then comes the hard or actually harder part, configuring the scan and making sure that you don't see any false results anymore. Remember that when we started the sidecar, we bound port 3000. This allows us to now actually check the site when we open a browser and go to port 3000 of our server. One of the notifications was about the FTP directory, so let's check whether this is actually a false positive or not. And we actually can open the directory. It exists. Not only that, this seriously looks like a vulnerability, all kinds of files, which probably should not be accessible. Here, you can immediately see the added value of using such a scanner. Let's now switch back to the console, check out a new branch, let's call it sidecar, and let's take a look at the Jenkinsfile, which I've already modified. So what exactly changed? The pipeline has got a few more stages. The first stage is still linting, linting of the Dockerfile. Then, the Dockerfile gets built, it gets pushed to the registry, and then we see a new stage. The sidecar is being launched. This is similar to what we did earlier. You detach the Docker container, you make sure is connected to the correct network, and you give it the name of the job and the build ID. This makes it easier to identify the sidecar. Then, after the sidecar has been launched, we start our scanning process. The Nikto command is almost the same as to command we used earlier with a few different parameters. First, we ensure that the reports directory exists, and then we start scanning. I've disabled the sitefiles plugins because that plugin generated a lot of false positives. Weeding out of false positives takes up a lot of time. With scanners, it's really important that you can properly configure them, and, fortunately, Nikto is one of them. And then rewrite our output to an HTML file, which you can publish in Jenkins. If there are any issues, if the scanner exits with a return code not equal to zero, then it will raise an alert for that stage. Something else new in the Jenkins pipeline is stopping of the sidecar container. We also stored the Nikto report, so that can be published in Jenkins. It bears repeating, this course is not about specifically tuning or tweaking tools. It's just to give you an example of how you can perform automated security tests and especially in a build pipeline. I'm going to exit bat. I'm going to add the Jenkinsfile to the repository. I'm going to commit the file and push it to our GitLab server. Now quickly jump over to Jenkins. Why quickly? Well, personally, I like to immediately see that the new branch will be detected and the build will start. And, yes, it picked up our new branch. Click on the branch name to see more details. And there you see it started the whole process. Now we have to wait for a while until the image has been built, the sidecar started, the sidecar scanned, stopped, and the pipeline will be ready. Fortunately, we can speed that up during video courses, and our scan has finished in 5 seconds. That's pretty fast. Is that okay? So I'm going to refresh the page and click on the scanned report at the left‑hand side. It didn't find a web server. That's strange. Well, if you think about it, actually no. What happened was that the build server started up a sidecar, and immediately after that, the scanner started scanning, but the server wasn't ready yet. And because the Docker image hasn't got a health check, we need to build that in manually in our build pipeline. We need to make sure that the application is actually ready to be scanned. Let's go back to the console. I prepared the necessary change, and by performing a git diff, you can see exactly what that change entails. Now the stage tries to perform a curl request, and it tries to do that until the server is ready to be served. So only after the server is accessible, then the scanning will start. We've built our own health check. Let's add this file to our Git repository, commit the change, push the change to our Git server, and then let's see whether it now succeeds. It picked up our changes and started building again. Fast‑forward to the Nikto stage, and now you can see that it actually takes a longer time to scan the application. So our health check has succeeded. The sidecar is up and running, and it properly scans it. And this is something you should be aware of when using this sidecar pattern. Ensure that the sidecar is actually running and operational before performing additional security tests against it. The security tests have completed, and, as you can see, the Nikto stage failed. It actually found issues. We've tried to weed out the false positives, so what is it actually complaining about? Let's open the new scan report from the left‑hand side menu. Even though we weeded out the false positives, these are actually valid findings, and we should take a look at those. It still detected the FTP directory, which we didn't close up. This was the Nikto demo, including the sidecar testing pattern. Let's hop over to the next demo where we will demonstrate the OWASP ZAP scanner.

Demo: Implementing OWASP ZAP and All Automated Security Tests
Welcome to the last demo of discourse. Time flies when you're having fun. In this demo, we're going to implement the whole testing pipeline. However, first we will implement OWASP ZAP, our dynamic analysis security testing tool. Then we'll look at its results and compare them with the previous demo where we used Nikto. And then the culmination of the course, we will be implementing all tools in our automated security testing pipeline. Let's start with implementing OWASP ZAP in our pipeline. Instead of branching off from master, we're now going to branch off from the sidecar branch, which we generated in the previous demo. The sidecar pattern is the same pattern we would like to use for using OWASP ZAP. Let's open up the Jenkinsfile again. I have already modified this file. And at the left‑hand side of the screen, you can see in the Git gutter, so to speak, the changes with regard to the previous version, the version that we used during the Nikto scan. Instead of using our tools‑image image, we're now using a third‑party image, owasp/zap2docker‑weekly. We're connecting it again to the network, and we're mapping /zap/wrk, its work directory, to our workspace. Then we're going to perform a so‑called baseline scan against our sidecar image. The scan is being executed with a number of parameters. We're not going to dive into those parameters right now. That's not what this course is about. What is important is that it will save the results to an HTML file, and we will publish that HTML file similar as to what we did with the Nikto and Dependency‑Check scan. Let's exit bat, add the Jenkinsfile to the repository, commit the file, and push it to our GitLab server. Then, let's quickly hop over to Jenkins to see whether it picked up our new branch and started scanning. Yes, it detected our branch. I'm going to click on OWASP ZAP branch, and here we can see the progress. Let's fast‑forward to the step where the OWASP ZAP scanning takes place. And yes, it starts scanning our sidecar. And now that it's finished, I'm going to refresh the page, and then we're going to see the new page at the left‑hand side. Let's open the page and compared the results with Nikto, our previous scan. Here we can see that ZAP picked up quite a number of issues. Whereas Nikto only scanned the server, the server configuration, OWASP ZAP goes in depth. It also picks up on server misconfiguration, but also examines the application itself. It performs dynamic application security testing. One of the issues it's found, for example, is suspicious comments in source code. So we've got this scanner up and running as well. Let's now go back to our console and implement all of the security tests we've seen so far. We're in the master branch, and I'm going to check out a new branch called automated‑security‑testing based off the master branch. Then, let's open the Jenkinsfile with bat once more and to see which changes I have secretly implemented. Let's go over some of the bigger changes. On top we're parameterizing the user ID and the group ID under which this Jenkins container is running. We need it in order to correct permissions from the dependency‑check container. First, we're going to check our code. We're using a linter here, then we're trying to detect new secrets, and then we're going to use a code quality metric system. We looked at all of these three elements in the module Automating Code Security Testing. And then we're going to test for insecure or outdated third‑party libraries, which we've looked at in the follow‑up module. Here we used the previously set variables, JENKINS_UID and JENKINS_GID, to enforce the correct permissions. Then, we're going to build the image, we're going to push the build image to the registry, and we're going to launch a sidecar from the image. Then, we're going to test our container as demonstrated in the module Automating Container Security Testing. The last two stages test the infrastructure. First, we're using Nikto, and them we're using the dynamic application security scanner, OWASP ZAP, which we just demonstrated. One important difference with the previous demos though is that we do not use hard quality gating. If a test fails, we only mark the build as unstable. We do not fail all tests. And at the end of our pipeline definition, we stop our sidecar container, and we publish all reports so that they're available within the build. Exit bat, clear the screen, add the Jenkinsfile to our repository, commit the file, push the file, and now let's look at our final build, the automated security testing branch where we implemented all tests. Let's click on the branch, and now we see the detailed build information or actually the detailed information about the tests that are being run. Our automated security tests will take up a long time, and that is also a reason why you should not do all of the tests after each commit. You might want to do the test asynchronously as we have discussed in the previous modules. Fortunately for us, we can fast‑forward the build, and there you can see that all the security tests have finished. And two of the tests, Dependency‑Check and nikto, actually found issues. And, therefore, the build has been marked as unstable. But the rest of the tests still continued. We didn't use hard quality gating. And that concludes the final demo of our course where we implemented all automated security testing. In the next clip, we're going to take a look at that workflow for infrastructure scanners.

Workflow and Conclusion of Infrastructure Scanning
The workflow for using infrastructure scanners. First, start the application as sidecar. This is the pattern we used during the demos. You want to make sure that that what you'd test resembles production as close as possible. Then, you scan the application, and then you spend a lot of time filtering out false positives. Remember the demo with Nikto? Well, it took a long time in order to remove those false positives and in order to tweak the scanner. You want to thoroughly configure the scanner so that it actually gives you useful results. Infrastructure scanning. It alerts you about security misconfigurations and scans the running application or infrastructure. The advantages. It finds security misconfigurations before the application is actually deployed to production. Compatibility. Well, most web interfaces can be scanned. However, user sessions are more difficult. So far, we only scanned the site as an anonymous user. We haven't demonstrated actually logging in and then scanning the site. This would yield much more interesting and useful results. Trialability. How easy is it to try out these scanners? It's moderately easy to add it to our pipeline. However, useful scans getting useful results, this takes up a long time. I cannot stress this part enough. For more information. I'd like to refer you to the Nikto website, as well as the OWASP ZAP website. In the next clip, we'll be summarizing the module, as well as the course. See you there.

Module and Course Summary
The module and course summary. Throughout the course, we've looked at code. We've looked at third‑party libraries. We've looked at containers. And in this module, we've looked at the infrastructure. If you look at the change frequency, code changes most often. And if you go all the way to the right, infrastructure changes least often. Let's now compare that with the scanning frequency. How often should you scan each of those parts? Well, code should be scanned most often, all the way to the right where it makes less sense to scan more often. Fortunately, this matches the trialability of all the tools we've mentioned in this course. It's relatively easy to trial code security scanning tools. It is relatively easy to implement tools to detect secrets in code, for example, all the way to containers and infrastructure, which can be more difficult to trial. To summarize our module and course, start with the quick wins. Look, for example, at detecting codes and secrets. Then, think beforehand what you expect from a tool. Some tools can lead to information overload. Therefore, make sure why you're using that specific tool. Does it make sense in your instance? Do not underestimate the time it takes for configuring tools. You want to make sure it actually gives you useful results, and tweaking can take up a long time. However, as you've hopefully seen in the last demo, a well‑configured automated security testing pipeline is very, very valuable. Automated security testing is a process, and it is not a product. This leads me to the end. Thank you so much for watching. My name is Peter Mosmans. I hope you have enjoyed the course. And should you have any questions, don't hesitate to use the discussion forum on the Pluralsight course page. And a sneak peak from the next course in this series. Next up, Maeve is very happy. This is all immediately applicable. However, how to implement those tools in my specific pipeline? Jennifer says, well, guess what the next course in the series is about? You have been watching the Performing DevSecOps Automated Security Testing course. The next course in this series will be Integrating or Implementing Automated Security Testing Tools in a Continuous Integration Pipeline. Hopefully see you there.
